{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from babel.dates import format_date\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:gray;\">Goal of this notebook</h2>\n",
    "\n",
    "- To scrape DW news articles on 4 main categories ('Politics', 'Business', 'Sports', 'Arts').\n",
    "\n",
    "(Note: The articles scraped have descending order of date.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CATEGORIES = ('Politics', 'Business', 'Sports', 'Arts')\n",
    "COLS = ['url', 'category', 'title', 'text', 'target_category'] # main dataframe columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for getting article url from anchor tag using xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(driver):\n",
    "    element = driver.find_element(By.XPATH,'.//a')\n",
    "    return element.get_attribute('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for getting article title from h2 header using xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(driver):\n",
    "    element = driver.find_element(By.XPATH,'.//h2')\n",
    "    return element.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility function for finding an element using xpath and returning it's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(driver,xpath):\n",
    "    data = np.nan\n",
    "    try:\n",
    "        data = driver.find_element(By.XPATH,xpath).text\n",
    "    except Exception as e:\n",
    "        print('element not found')\n",
    "    finally:\n",
    "        return data        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility function for finding elements using xpath and returning list of their texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_data(driver, xpath):\n",
    "    data = []\n",
    "    try:\n",
    "        elems = driver.find_elements(By.XPATH,xpath)\n",
    "        for elem in elems:\n",
    "            data.append(elem.text)\n",
    "    except Exception as e:\n",
    "        print('elements not found')\n",
    "    finally:\n",
    "        return data   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for setting all articles data into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data_dict(url,category,title,text,target_category):\n",
    "    data_dict = {'url':[url],'category':[category],\\\n",
    "                 'title':[title],'text':[text],'target_category':[target_category]}\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to scrape articles url and title based on the url which has search item as one of the target categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_list(url,target_category):\n",
    "\n",
    "    # set up selenium driver in a Chrome browser environment (make sure you have Chrome installed)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 30) # make the driver wait for 30 seconds\n",
    "    \n",
    "    articles_all = [] # initialize an empty list to store articles scraped\n",
    "    try:\n",
    "        # check if all search results xpath of articles are found or not\n",
    "        elements_present = EC.presence_of_all_elements_located((By.XPATH,'//div[@id=\"searchResult\"]//div[@class=\"searchResult\"]'))\n",
    "        wait.until(elements_present)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('id, xpath or class name of the element is not found!')\n",
    "        driver.quit()\n",
    "\n",
    "    try:\n",
    "        # find all elements in search results (articles)\n",
    "        articles = driver.find_elements(By.XPATH,'//div[@id=\"searchResult\"]//div[@class=\"searchResult\"]')\n",
    "        \n",
    "        for article in articles:\n",
    "            article_url = get_url(article)\n",
    "            title = get_title(article)\n",
    "            articles_all.append((article_url,title,target_category))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Search results error!')\n",
    "        driver.quit()\n",
    "    \n",
    "    driver.quit()\n",
    "    return articles_all\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to scrape article's main text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article_text(articles):\n",
    "    \n",
    "    # set up selenium driver in a Chrome browser environment (make sure you have Chrome installed)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    # main dataframe where all articles and its features are appended\n",
    "    main_df = pd.DataFrame(columns=COLS)\n",
    "\n",
    "    # loop through all articles and scrape the actual text by going throught it's url which was stored in the above step\n",
    "    for item in articles:\n",
    "        # url, title are already scraped from the search page\n",
    "        url = item[0]\n",
    "        title = item[1]\n",
    "        target_category = item[2]\n",
    "\n",
    "        driver.get(item[0])\n",
    "        driver.implicitly_wait(20) # configure implicit wait for 20s since multiple elements are scraped\n",
    "        \n",
    "        # get category from the xpath\n",
    "        elems = driver.find_elements(By.XPATH,'//div[@class=\"sc-iJfdHH sc-iUeHef sc-lbNsEr dEMmVC imciYi imfbMx sc-fkekHa hFHcoo kicker sc-iuWDFx fWEOHE\"]//span')\n",
    "        if len(elems) > 0:\n",
    "            category = elems[0].text\n",
    "        else:\n",
    "            category = np.nan\n",
    "\n",
    "        # get all paragraph texts and join them with a new line\n",
    "        texts = get_multi_data(driver, '//div[@class=\"sc-iJfdHH sc-iUeHef sc-hjcAwj dEMmVC imciYi eGhiVL sc-kMTSIo GtYvC rich-text has-italic\"]//p')\n",
    "        if len(texts)>0:\n",
    "            text = '\\n'.join(texts)\n",
    "        else:\n",
    "            text = np.nan\n",
    "\n",
    "        # create a row of dictionary to concat in final dataframe\n",
    "        data_dict = set_data_dict(url,category,title,text,target_category)\n",
    "        article_df = pd.DataFrame(data_dict)\n",
    "        main_df = pd.concat([main_df,article_df],ignore_index=True)\n",
    "\n",
    "    driver.quit()\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:gray;\">SCRAPE ARTICLES URL AND TITLE  BY SEARCHING TARGET CATEGORIES STRINGS ON A LOOP</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of articles scraped = 800\n"
     ]
    }
   ],
   "source": [
    "articles_all = []\n",
    "for target_category in TARGET_CATEGORIES:\n",
    "    search_url = f\"https://www.dw.com/search/?languageCode=en&&contentType=ARTICLE&item={target_category}&searchNavigationId=9097-30688&sort=DATE&resultsCounter=200\"\n",
    "    articles_single_category = scrape_articles_list(search_url, target_category)\n",
    "    articles_all.extend(articles_single_category)\n",
    "\n",
    "print(f'No of articles scraped = {len(articles_all)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:gray;\">SCRAPE THE TEXT OF THE ARTICLE BY GOING TO THE URL OF THE ARTICLE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_article_text(articles_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>target_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.dw.com/en/germany-undeterred-by-gl...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Germany undeterred by global turmoil — Scholz ...</td>\n",
       "      <td>In his New Year's message, German Chancellor O...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.dw.com/en/taiwan-presidential-cand...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Taiwan: Presidential candidates debate in shad...</td>\n",
       "      <td>Taiwan's presidential candidates argued over w...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.dw.com/en/emboldened-iran-silences...</td>\n",
       "      <td>HUMAN RIGHTS</td>\n",
       "      <td>Emboldened Iran silences critics as world look...</td>\n",
       "      <td>As least 690 prisoners were executed in Iran i...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.dw.com/en/berlin-prepares-for-anot...</td>\n",
       "      <td>SOCIETY</td>\n",
       "      <td>Berlin prepares for another rowdy New Year's E...</td>\n",
       "      <td>In Germany, Christmas is all about contemplati...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.dw.com/en/albania-former-pm-put-un...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>Albania: Former PM put under house arrest in c...</td>\n",
       "      <td>Albania's right-wing opposition leader Sali Be...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url      category  \\\n",
       "0  https://www.dw.com/en/germany-undeterred-by-gl...      POLITICS   \n",
       "1  https://www.dw.com/en/taiwan-presidential-cand...      POLITICS   \n",
       "2  https://www.dw.com/en/emboldened-iran-silences...  HUMAN RIGHTS   \n",
       "3  https://www.dw.com/en/berlin-prepares-for-anot...       SOCIETY   \n",
       "4  https://www.dw.com/en/albania-former-pm-put-un...         CRIME   \n",
       "\n",
       "                                               title  \\\n",
       "0  Germany undeterred by global turmoil — Scholz ...   \n",
       "1  Taiwan: Presidential candidates debate in shad...   \n",
       "2  Emboldened Iran silences critics as world look...   \n",
       "3  Berlin prepares for another rowdy New Year's E...   \n",
       "4  Albania: Former PM put under house arrest in c...   \n",
       "\n",
       "                                                text target_category  \n",
       "0  In his New Year's message, German Chancellor O...        Politics  \n",
       "1  Taiwan's presidential candidates argued over w...        Politics  \n",
       "2  As least 690 prisoners were executed in Iran i...        Politics  \n",
       "3  In Germany, Christmas is all about contemplati...        Politics  \n",
       "4  Albania's right-wing opposition leader Sali Be...        Politics  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 800 entries, 0 to 799\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   url              800 non-null    object\n",
      " 1   category         794 non-null    object\n",
      " 2   title            800 non-null    object\n",
      " 3   text             794 non-null    object\n",
      " 4   target_category  800 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the data as a csv file so no scraping for 31 mins should be redone\n",
    "# this csv file would be used as a dataset in task2_modeling.ipynb notebook\n",
    "df.to_csv('dw_articles.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://www.dw.com/search/?languageCode=en"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
